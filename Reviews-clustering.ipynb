{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reviews-clustering.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP9WesTWuvAPcWeZ301DUBC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import string\n","from string import digits\n","import numpy as np\n","import nltk\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.cluster import DBSCAN\n","from nltk.tokenize import RegexpTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.neighbors import NearestNeighbors\n","from matplotlib import pyplot as plt\n","from google.colab import drive\n","drive.mount('/drive', force_remount=True)\n","\n","# DOWNLOADS\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download(\"stopwords\")\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","sw = stopwords.words(\"english\")\n","\n","########## CONFIG ##########\n","notebook_drive_path = '/drive/MyDrive/Colab Notebooks/Hermes_reviews_experiment/'\n","ONLY_NOUNS_FOR_CLUSTERING = True\n","MAX_DF = 1.0\n","MIN_DF = 1\n","EPS = 0.8\n","MIN_SAMPLES = 2\n","TARGET_TOPICS = 3\n","########## CONFIG ##########"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUWLMlAg_LVf","executionInfo":{"status":"ok","timestamp":1655282986078,"user_tz":-120,"elapsed":5358,"user":{"displayName":"Jonathan Simeone","userId":"01736008147765826623"}},"outputId":"f2a3b470-2734-4e8c-9916-a5c257df824b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","def filter_nouns(sentence):\n","  tokenized = nltk.word_tokenize(sentence)\n","  nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if(pos[:2] == 'NN')]\n","  return nouns\n","\n","def clean(sentence):\n","    no_punctuation = re.sub(r\"[,.'’‘\\\"“”:;/*()%@#?§£Є°#\\\\^\\+\\-ç!&$]+\\ *\", \" \", sentence.lower())\n","    no_escape = re.sub(r'(\\r\\n|\\n|\\r|\\t)', ' ', no_punctuation)\n","    no_urls = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', ' ', no_escape)\n","    no_emoji = re.sub(r\"[\"u\"\\U0001F600-\\U0001F64F\"\n","                  u\"\\U0001F300-\\U0001F5FF\"  \n","                  u\"\\U0001F680-\\U0001F6FF\"  \n","                  u\"\\U0001F1E0-\\U0001F1FF\" \n","                  u\"\\U00002702-\\U000027B0\"\n","                  u\"\\U000024C2-\\U0001F251\"\n","                  \"]+\", \" \", no_urls, flags=re.UNICODE)\n","    only_alpha = re.sub(\"[^a-z]+\", \" \", no_emoji)\n","    no_digits = only_alpha.translate(str.maketrans('', '', digits))\n","    return no_digits\n","\n","\n","def tokenize(sentence):\n","    cleaned = clean(sentence)\n","\n","    if ONLY_NOUNS_FOR_CLUSTERING:\n","      tokens = filter_nouns(cleaned)\n","    else:\n","      tokens = cleaned.split()\n","\n","\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in sw]\n","    tokens = [stemmer.stem(word) for word in tokens]\n","    return tokens\n","\n","def tokenize_topics(sentence):\n","    cleaned = clean(sentence)\n","    only_nouns = filter_nouns(cleaned)\n","    tokens = [word for word in only_nouns if word not in sw]\n","    return tokens\n","\n","\n","print('1) Loading CSV...')\n","df = pd.read_csv(notebook_drive_path + 'test.csv', index_col=0)\n","\n","print('2) Extracting tokens...')\n","cv = CountVectorizer(min_df=MIN_DF, max_df=MAX_DF, analyzer=tokenize)\n","x_cv = cv.fit_transform(df['Review']).toarray()\n","\n","print('\\nTokens vocabulary:')\n","print(cv.get_feature_names_out())\n","print('\\n')\n","\n","print('3) Generating TF-IDF matrix...\\n')\n","tfidf_converter = TfidfTransformer()\n","x_tf_matrix = tfidf_converter.fit_transform(x_cv).toarray()\n","\n","print('Vocabulary Size : ', len(cv.get_feature_names_out()))\n","print('Shape of Matrix : ', x_tf_matrix.shape)\n","\n","print('\\n4) Clustering...')\n","\n","clusterer = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric='cosine', n_jobs=-1)\n","\n","clusterer_name = 'DBSCAN'\n","print('\\nClusterer: ' + clusterer_name)\n","result = clusterer.fit_predict(x_tf_matrix)\n","\n","print('\\n5)Generating clusters dataframe...')\n","clusters_df = pd.DataFrame(columns=['Review', 'Cluster'], data=zip(df['Review'], result))\n","print('\\n')\n","print(clusters_df['Cluster'].value_counts())\n","print('\\n')\n","print('6) Saving clusters csv...')\n","clusters_df.to_csv(notebook_drive_path + clusterer_name + '_test_clusters.csv')\n","\n","print('\\n7) Analizing clusters for topics extraction...')\n","\n","clusters = clusters_df['Cluster'].unique()\n","print('\\nNum of clusters (with possible noise cluster = -1):', len(clusters))\n","print('Clusters labels: ', clusters)\n","print('\\n')\n","\n","print('CLUSTERS:')\n","\n","clusters_df['Cluster_topics'] = None\n","\n","clusters_topics = dict()\n","\n","for cluster in clusters:\n","  topics = []\n","\n","  print('---------------------')\n","  print('Cluster ' + (str(cluster) + '\\n' if cluster != -1 else 'NOISY:\\n'))\n","  cluster_reviews = clusters_df.query(\"Cluster == \" + str(cluster))\n","  reviews = cluster_reviews['Review'].tolist()\n","\n","  print('Tokenizing topics...\\n')\n","  tfidf = TfidfVectorizer(tokenizer = tokenize_topics)\n","  data = tfidf.fit_transform(reviews) \n","  terms = tfidf.get_feature_names_out()\n","  print('\\nTopics:')\n","  print(terms)\n","\n","  print('\\nApplying LDA...\\n')\n","  model=LatentDirichletAllocation(n_components=1)\n","  model.fit_transform(data)\n","  lda_components=model.components_\n","\n","  for index, component in enumerate(lda_components):\n","      zipped = zip(terms, component)\n","      top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:TARGET_TOPICS]\n","      top_terms_list=list(dict(top_terms_key).keys())\n","      topics = topics + top_terms_list\n","     \n","  print(\"\\nTOPICS: \", topics)\n","  clusters_topics[cluster] = topics\n","  print('---------------------')\n","\n","clusters_df['Cluster_topics'] = clusters_df.apply(lambda row: clusters_topics[row['Cluster']], axis = 1)\n","\n","print('\\n8) Saving labeled clusters csv...')\n","clusters_df.to_csv(notebook_drive_path + clusterer_name + '_test_clusters_LABELED.csv')\n","\n","print('\\nε estimation\\n')\n","\n","#Calculate the average distance between each point in the data set and its nearest neighbors\n","neighbors = NearestNeighbors(n_neighbors=MIN_SAMPLES,  metric='cosine')\n","neighbors_fit = neighbors.fit(x_tf_matrix)\n","distances, indices = neighbors_fit.kneighbors(x_tf_matrix)\n","distances = np.sort(distances, axis=0)\n","distances = distances[:,1]\n","plt.plot(distances)\n","plt.show()\n","\n","print('\\nClusters:')\n","pd.set_option('display.max_colwidth', None)\n","df = pd.read_csv(notebook_drive_path + 'DBSCAN_test_clusters_LABELED.csv', index_col=0)\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjxUTcAL4bx_","outputId":"698ebfb7-33a5-41db-e6e0-4bfe517b6f93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1) Loading CSV...\n","2) Extracting tokens...\n"]}]}]}